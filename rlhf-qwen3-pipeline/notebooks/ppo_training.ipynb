{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training Notebook\n",
    "\n",
    "This notebook implements the training of the policy model using Proximal Policy Optimization (PPO) based on the reward model trained from human feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.utils.config import load_config\n",
    "from src.models.policy_model import PolicyModel\n",
    "from src.models.reward_model import RewardModel\n",
    "from src.training.ppo_trainer import PPOTrainer\n",
    "\n",
    "# Load configuration for PPO training\n",
    "ppo_config = load_config('configs/ppo_config.yaml')\n",
    "\n",
    "# Initialize the reward model\n",
    "reward_model = RewardModel(ppo_config['reward_model'])\n",
    "\n",
    "# Initialize the policy model\n",
    "policy_model = PolicyModel(ppo_config['policy_model'])\n",
    "\n",
    "# Initialize the PPO trainer\n",
    "ppo_trainer = PPOTrainer(policy_model, reward_model, ppo_config)\n",
    "\n",
    "# Start the training process\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we set up the PPO training pipeline for the policy model. We loaded the necessary configurations, initialized the models, and started the training process. After training, the policy model will be able to maximize rewards based on the feedback received."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}